
# How I am learning LLMs & NLP in 2024
- Study `Natural Language Processing`

## Improvements from '2023 Learning Experience'
Details: [last year's Computer Vision](https://github.com/ajinkyakolhe112/Mastering-Deep-Learning-in-2023)
Things I am improving
  - Focus on high level libraries like `transformers` and `diffusers` over `pytorch`
    - Advantage: Few lines to have end to end pipeline
    - Advantage: Getting to solution quickly instead of 2 weeks of implementing from scratch
  - Do more Kaggle competitions compared to courses.    (10 courses & 3 competitions last year, Aiming for 10+ competitions this year)
  - Visualize model internals to understand it better.  (Didn't do this part last year)
  - Understand maths aspect of neural networks.         (Didn't do this part last year.)
  - Continue coding in pytorch, build code cookbook for experimentation

## NLP - NLP Landscape's two entry paths
- NLP Landscape has two entry points, one slow, long & easier path & other short but steep path
  - Long & Slow path:   DL Basics -> Simple NN -> CNN -> RNN -> LSTM -> Word2Vec -> Attention -> LLMs
  - Short & Steep path: DL Basics -> **Attention is all you need** -> LLMs
    - Everything in NLP & CV is building on top of this single paper. Highest citations, higest used architecture, is most varied kinds of problems.
    - Understand this thoroughly, because everything builds on this

## Quarter 1 - Things Learned

### NLP - 5 STAR RESOURCES
|   Type                |    Details                        | Progress                            |
| ---------             | ----------                        | --------------------------------    |
1: Course               | Huggingface NLP                   | ![](https://geps.dev/progress/100)  |
2: Kaggle Competition   | Disaster Tweet Classification     | ![](https://geps.dev/progress/100)  |
3: Research Paper       | Attention is all you need         | ![](https://geps.dev/progress/100)  |
4: Research Paper       | One Model to learn them all                                           | ![](https://geps.dev/progress/100)  |
5: Youtube Video        | 3Blue1Brown: Attention in transformers, visually explained            | ![](https://geps.dev/progress/100)  |
6: Youtube Video        | 3Blue1Brown: But what is a GPT? Visual intro to transformers          | ![](https://geps.dev/progress/100)  |
7: Youtube Video        | Campus X: Epic History of Large Language Models                       | ![](https://geps.dev/progress/100)  |
8: Youtube Video        | Campus X: Self Attention          | ![](https://geps.dev/progress/100)  |
9: Youtube Video        | Campus X: Attention               | ![](https://geps.dev/progress/100)  |

### Computer Vision - 5 STAR RESOURCES
|   Type                |    Details                        | Progress                            |
| ---------             | ----------                        | --------------------------------    |
1: Course               | Huggingface timm                    | ![](https://geps.dev/progress/100)  |
2: Course               | Huggingface diffusers               | ![](https://geps.dev/progress/100)  |

### Backlog of Resources: 
Important will be prioratized on need basis
#### 1. Courses
   2. [CS25: Transformers United](https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html)
   3. [Gen AI with LLM](https://www.coursera.org/learn/generative-ai-with-llms)
   4. [CS224n Course](https://web.stanford.edu/class/cs224n/index.html#coursework)
   5. [Deeplearning.ai NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)

#### 2. Kaggle Competitions
   - https://www.kaggle.com/competitions/feedback-prize-2021
   - https://www.kaggle.com/competitions/kaggle-llm-science-exam
   - https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries
   - https://www.kaggle.com/competitions/feedback-prize-english-language-learning/
   - https://www.kaggle.com/competitions/feedback-prize-effectiveness

#### 3. Research Papers
   1. Attention is all you need, GPT, Bert
   2. https://github.com/terryum/awesome-deep-learning-papers
   3. [Yannic Kilcher - NLP Research Papers](https://www.youtube.com/watch?v=u1_qMdb0kYU&list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu)
   4. Articles  
     1. https://sebastianraschka.com/blog/2023/llm-reading-list.html
     2. https://magazine.sebastianraschka.com/p/understanding-large-language-models
     3. https://github.com/keon/awesome-nlp