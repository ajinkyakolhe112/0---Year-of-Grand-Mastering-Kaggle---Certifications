
# How I am learning LLMs & NLP in 2024
- Study `Natural Language Processing`

## Improvements from '2023 Learning Experience'
Details: [last year's Computer Vision](https://github.com/ajinkyakolhe112/Mastering-Deep-Learning-in-2023)
Things I am improving
  - Focus on mastering one library which has standarized all transformer models, for coding of end to end projects. (Last year's pain point)
  - Do more Kaggle competitions compared to courses. (10 courses & 3 competitions last year, Aiming for 10+ competitions this year)
  - Visualize model internals to understand it better. (Didn't do this part last year)
  - Understand maths aspect of neural networks. (Didn't do this part last year.)
  - Continue coding in pytorch, build code cookbook for experimentation

## NLP Landscape's two entry paths
- NLP Landscape has two entry points, one slow, long & easier path & other short but steep path
  - Long & Slow path:   DL Basics -> Simple NN -> CNN -> RNN -> LSTM -> Word2Vec -> Attention -> LLMs
  - Short * Steep path: DL Basics -> **Attention is all you need** -> LLMs
    - Everything in NLP & CV is building on top of this single paper. Highest citations, higest used architecture, is most varied kinds of problems.
    - Understand this thoroughly, because everything builds on this

## Short but Steep Path: Base Essentials
|   Type                |    Details                        | Progress                          |
| ---------             | ----------                        | --------------------------------  |
1: Course               | Huggingface NLP                   | ![](https://geps.dev/progress/10) |
2: Kaggle Competition   | Disaster Tweet Classification     | ![](https://geps.dev/progress/01)  |
3: Research Paper       | Attention is all you need         | ![](https://geps.dev/progress/10)  |

### Backlog of Resources: Important will be prioratized on need basis
#### 1. Courses
   1. Huggingface NLP Course
   2. [CS25: Transformers United](https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html)
   3. [Gen AI with LLM](https://www.coursera.org/learn/generative-ai-with-llms)
   4. [CS224n Course](https://web.stanford.edu/class/cs224n/index.html#coursework)
   5. [Deeplearning.ai NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)

#### 2. Kaggle Competitions
   - https://www.kaggle.com/competitions/feedback-prize-2021
   - https://www.kaggle.com/competitions/kaggle-llm-science-exam
   - https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries
   - https://www.kaggle.com/competitions/feedback-prize-english-language-learning/
   - https://www.kaggle.com/competitions/feedback-prize-effectiveness

#### 3. Research Papers
   1. Attention is all you need, GPT, Bert
   2. https://github.com/terryum/awesome-deep-learning-papers
   3. [Yannic Kilcher - NLP Research Papers](https://www.youtube.com/watch?v=u1_qMdb0kYU&list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu)
   4. Articles  
     1. https://sebastianraschka.com/blog/2023/llm-reading-list.html
     2. https://magazine.sebastianraschka.com/p/understanding-large-language-models
     3. https://github.com/keon/awesome-nlp
